{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 1: Importing neccacary Libararies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import os\n",
    "conn = sqlite3.connect('iotaDB.db')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Step1: Read the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('iota_tx_2024/iota_tx/IOTA_1year_tx_data2.csv', header=0)\n",
    "print(df.columns)  # to check column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h3>Step 2: Cleaning the Dataset</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>2.1 Identifying missing values, deleting the rows, saving in a new directory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 2.1.1 Identifying \"not found\" values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notfound_values(df):\n",
    "    # Convert DataFrame to numpy array for fast operation\n",
    "    data_array = df.values\n",
    "    \n",
    "    # Vectorized comparison over the numpy array\n",
    "    is_not_found = (data_array == 'Not found')\n",
    "    \n",
    "    # Use numpy to check each row\n",
    "    contains_not_found = np.any(is_not_found, axis=1)\n",
    "    \n",
    "    # Count the True values for rows containing 'Not found'\n",
    "    not_found_count = np.sum(contains_not_found)\n",
    "    \n",
    "    print(f\"Total rows with 'Not found': {not_found_count}\")\n",
    "    return not_found_count\n",
    "\n",
    "\n",
    "notfound_count = notfound_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 2.1.2 Identifying \"missing\" values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values (df):\n",
    "  missing_values = df.isna().sum()\n",
    "  print(\"Missing values in each column:\")\n",
    "  print(missing_values)\n",
    "  return missing_values\n",
    "\n",
    "missing_values_count = missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 2.1.4 Cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check for 'Not Found' across the entire DataFrame\n",
    "mask = (df != 'Not found').all(axis=1)\n",
    "\n",
    "# Count rows before filtering\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Apply the mask to filter out rows with 'Not Found'\n",
    "cleaned_df = df[mask]\n",
    "\n",
    "# Count rows after filtering\n",
    "final_row_count = len(cleaned_df)\n",
    "rows_deleted = initial_row_count - final_row_count\n",
    "\n",
    "# Output the number of rows deleted\n",
    "print(f\"Total rows deleted: {rows_deleted}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all rows that have any missing values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaned_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2.2 Adjusting the datatypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  # Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display the first 4 rows in the database\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makeing sure about the right datatype\n",
    "print(type(df['timestamp'].iloc[0]))\n",
    "print(type(df['input_amounts_x'].iloc[0]))\n",
    "print(type(df['input_addresses_x'].iloc[0]))\n",
    "print(type(df['output_addresses_y'].iloc[0]))\n",
    "print(type(df['output_amounts_y'].iloc[0]))\n",
    "df.iloc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 2.1.3.1 Handling the datatype of input_adresses_x and output_addresses_y // Changing all the adresses with more than one input from strings to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string_to_list(s):\n",
    "    try:\n",
    "        if s is None or isinstance(s, list):\n",
    "            return []  # Handle None or already processed lists\n",
    "        parsed_list = ast.literal_eval(s)\n",
    "\n",
    "        if s is None or s == \"Not found\":\n",
    "            print(f\"Warning: Skipping due to None or Not found input at row : {s}\")\n",
    "            return []\n",
    "        return parsed_list\n",
    "    except (ValueError, SyntaxError, TypeError) as e:\n",
    "        print(f\"Error parsing or converting {s}: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input_addresses_x'] = df['input_addresses_x'].apply(parse_string_to_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['output_addresses_y'] = df['output_addresses_y'].apply(parse_string_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df['input_addresses_x'].iloc[2]))\n",
    "print(df['input_addresses_x'].iloc[1])\n",
    "\n",
    "print(type(df['output_addresses_y'].iloc[2]))\n",
    "print(df['output_addresses_y'].iloc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 2.1.3.1 Handling the datatype of input_amount_x and output_amount_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_float_list(s):\n",
    "    try:\n",
    "        # Check for 'None' or the specific 'Not found' string\n",
    "        if s is None or s == \"Not found\":\n",
    "            print(f\"Warning: Skipping due to None or Not found input at row : {s}\")\n",
    "            return []\n",
    "        \n",
    "        # Process the list if 's' is already a list\n",
    "        if isinstance(s, list):\n",
    "            return [float(item) for item in s]  # Convert each item to float\n",
    "        \n",
    "        # If 's' is a string, attempt to parse it as a literal list\n",
    "        if isinstance(s, str):\n",
    "            parsed_list = ast.literal_eval(s)\n",
    "            return [float(item) for item in parsed_list]\n",
    "        \n",
    "        # Log any unexpected data types\n",
    "        print(f\"Unexpected data type : {type(s)} with value {s}\")\n",
    "        return []\n",
    "\n",
    "    except (ValueError, SyntaxError, TypeError) as e:\n",
    "        print(f\"Error parsing or converting  {s}. Error: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['output_amounts_y'] = df['output_amounts_y'].apply(parse_float_list)\n",
    "df['input_amounts_x'] = df['input_amounts_x'].apply(parse_float_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makeing sure about the right datatype\n",
    "print(type(df['input_amounts_x'].iloc[0]))\n",
    "print(type(df['input_amounts_x'].iloc[0][0]))\n",
    "print(type(df['input_addresses_x'].iloc[0]))\n",
    "\n",
    "print(type(df['output_amounts_y'].iloc[0]))\n",
    "print(type(df['output_amounts_y'].iloc[0][0]))\n",
    "print(type(df['output_addresses_y'].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makeing sure about the right datatype\n",
    "\n",
    "print(type(df['input_addresses_x'].iloc[1]))\n",
    "print(df['input_addresses_x'].iloc[1])\n",
    "\n",
    "print(type(df['input_amounts_x'].iloc[1]))\n",
    "print(df['input_amounts_x'].iloc[1])\n",
    "\n",
    "\n",
    "\n",
    "print(type(df['output_addresses_y'].iloc[1]))\n",
    "print(df['output_addresses_y'].iloc[1])\n",
    "\n",
    "print(type(df['output_amounts_y'].iloc[1]))\n",
    "print(df['output_amounts_y'].iloc[1])\n",
    "\n",
    "print(type(df['output_amounts_y'].iloc[1][0]))\n",
    "print(df['output_amounts_y'].iloc[1][0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step 4: Connecting to the database and inserting the cleaned and adjusted Dataframe</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 4.1 Saving the list in json format to store in the Database (!List can not be stored in the database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "for col in df.columns:\n",
    "  if df[col].apply(lambda x: isinstance (x,list)).any():\n",
    "    df[col] = df[col].apply(json.dumps)\n",
    "\n",
    "print(\"DataFrame Structure:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 4.2 Creating the Database and createing the Table for the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with corrected column names and data types\n",
    "conn = sqlite3.connect('iotaDB.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"DROP TABLE Transactions\")\n",
    "c.execute('''\n",
    "CREATE TABLE Transactions (\n",
    "    transaction_id ,\n",
    "    block_index ,\n",
    "    input_addresses_x ,\n",
    "    input_amounts_x ,\n",
    "    output_addresses_y ,\n",
    "    output_amounts_y ,\n",
    "    output_timestamp \n",
    ")\n",
    "''')\n",
    "\n",
    "# Use a default single insert statement per row\n",
    "df.to_sql('Transactions', conn, if_exists='replace', index=False, method=None)\n",
    "\n",
    "\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with JSON strings in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verification of List in the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "# Example database fetch code\n",
    "conn = sqlite3.connect('iotaDB.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Query to fetch serialized data\n",
    "c.execute(\"\"\"\n",
    "SELECT input_addresses_x, output_addresses_y, input_amounts_x, output_amounts_y \n",
    "FROM Transactions \n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "rows = c.fetchall()\n",
    "\n",
    "# Deserialize the JSON string back into Python lists for all four columns\n",
    "addresses_and_amounts = [{\n",
    "    'input_addresses': json.loads(row[0]) if row[0] else None,\n",
    "    'output_addresses': json.loads(row[1]) if row[1] else None,\n",
    "    'input_amounts': json.loads(row[2]) if row[2] else None,\n",
    "    'output_amounts': json.loads(row[3]) if row[3] else None\n",
    "} for row in rows]\n",
    "\n",
    "# Output the deserialized data along with types\n",
    "for idx, item in enumerate(addresses_and_amounts, start=1):\n",
    "    print(f\"Record {idx} - Input Addresses: {item['input_addresses']} (type: {type(item['input_addresses'])})\")\n",
    "    print(f\"Record {idx} - Output Addresses: {item['output_addresses']} (type: {type(item['output_addresses'])})\")\n",
    "    print(f\"Record {idx} - Input Amounts: {item['input_amounts']} (type: {type(item['input_amounts'])})\")\n",
    "    print(f\"Record {idx} - Output Amounts: {item['output_amounts']} (type: {type(item['output_amounts'])})\")\n",
    "\n",
    "# Close the database connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('iotaDB.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "# Execute query to fetch serialized data\n",
    "c.execute(\"\"\"\n",
    "SELECT input_amounts_x \n",
    "FROM Transactions \n",
    "LIMIT 10\n",
    "\"\"\")\n",
    "rows = c.fetchall()\n",
    "\n",
    "# Deserialize the JSON string back into Python lists\n",
    "input_amounts = [json.loads(row[0]) if row[0] else None for row in rows]\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check and print the type of each element in the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1 - Input Amounts Types:\n",
      "  Element 1: 500000000.0 (type: <class 'float'>)\n",
      "Record 2 - Input Amounts Types:\n",
      "  Element 1: 60000000.0 (type: <class 'float'>)\n",
      "  Element 2: 76000000.0 (type: <class 'float'>)\n",
      "Record 3 - Input Amounts Types:\n",
      "  Element 1: 200000000.0 (type: <class 'float'>)\n",
      "Record 4 - Input Amounts Types:\n",
      "  Element 1: 25000000.0 (type: <class 'float'>)\n",
      "Record 5 - Input Amounts Types:\n",
      "  Element 1: 494500000.0 (type: <class 'float'>)\n",
      "Record 6 - Input Amounts Types:\n",
      "  Element 1: 29891752444.0 (type: <class 'float'>)\n",
      "Record 7 - Input Amounts Types:\n",
      "  Element 1: 22994863356.0 (type: <class 'float'>)\n",
      "Record 8 - Input Amounts Types:\n",
      "  Element 1: 1000000.0 (type: <class 'float'>)\n",
      "Record 9 - Input Amounts Types:\n",
      "  Element 1: 4501456962.0 (type: <class 'float'>)\n",
      "Record 10 - Input Amounts Types:\n",
      "  Element 1: 0.0 (type: <class 'float'>)\n"
     ]
    }
   ],
   "source": [
    "# Check and print the type of each element in the lists\n",
    "for idx, amounts in enumerate(input_amounts, start=1):\n",
    "    if amounts:\n",
    "        print(f\"Record {idx} - Input Amounts Types:\")\n",
    "        for i, amount in enumerate(amounts):\n",
    "            print(f\"  Element {i+1}: {amount} (type: {type(amount)})\")\n",
    "    else:\n",
    "        print(f\"Record {idx} - No input amounts available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
